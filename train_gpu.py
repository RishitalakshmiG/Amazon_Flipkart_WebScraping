"""
SBERT Fine-Tuning with GPU Support
Automatically detects and uses GPU if available (10-50x faster)
"""

import os
import json
import logging
from datetime import datetime
import torch

# Suppress warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def check_gpu():
    """Check GPU availability and setup"""
    logger.info(f"\n{'='*80}")
    logger.info("CHECKING GPU AVAILABILITY")
    logger.info(f"{'='*80}\n")
    
    # Check CUDA
    cuda_available = torch.cuda.is_available()
    logger.info(f"CUDA Available:        {cuda_available}")
    
    if cuda_available:
        logger.info(f"CUDA Version:          {torch.version.cuda}")
        logger.info(f"Device Count:          {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            device_name = torch.cuda.get_device_name(i)
            device_props = torch.cuda.get_device_properties(i)
            memory_gb = device_props.total_memory / 1e9
            logger.info(f"GPU {i}:                {device_name} ({memory_gb:.1f} GB)")
        
        # Set device
        torch.cuda.set_device(0)
        current_device = torch.cuda.current_device()
        logger.info(f"Using GPU Device:      {current_device} ({torch.cuda.get_device_name(current_device)})")
        
        device = torch.device(f"cuda:{current_device}")
        logger.info(f"PyTorch Device:        {device}\n")
        
        return device, True
    else:
        logger.warning("\n‚ö†Ô∏è  No GPU detected - training will use CPU (slower)")
        logger.info("To enable GPU:")
        logger.info("  1. Install NVIDIA CUDA Toolkit (https://developer.nvidia.com/cuda-downloads)")
        logger.info("  2. Install cuDNN (https://developer.nvidia.com/cudnn)")
        logger.info("  3. Reinstall PyTorch with CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        logger.info()
        device = torch.device("cpu")
        logger.info(f"Using Device:          CPU (slower)\n")
        
        return device, False

def main():
    logger.info(f"\n{'='*80}")
    logger.info("SBERT FINE-TUNING WITH 11,695 TRAINING PAIRS (GPU-ENABLED)")
    logger.info(f"{'='*80}\n")
    
    # Check GPU first
    device, gpu_available = check_gpu()
    
    # Load training data
    logger.info("Loading 11,695 training pairs...")
    examples = []
    with open('training_data.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            examples.append({
                'sent1': data['sentence1'],
                'sent2': data['sentence2'],
                'label': float(data['label'])
            })
    
    similar = sum(1 for ex in examples if ex['label'] >= 0.5)
    dissimilar = len(examples) - similar
    
    logger.info(f"‚úì Loaded {len(examples):,} pairs")
    logger.info(f"  Similar (1.0):    {similar:,} ({similar*100/len(examples):.1f}%)")
    logger.info(f"  Dissimilar (0.0): {dissimilar:,} ({dissimilar*100/len(examples):.1f}%)\n")
    
    # Import sentence-transformers
    logger.info("Importing sentence-transformers library...")
    try:
        from sentence_transformers import SentenceTransformer, InputExample, losses, models
        from torch.utils.data import DataLoader
        import random
        logger.info("‚úì Imports successful\n")
    except ImportError as e:
        logger.error(f"‚úó Import failed: {e}")
        raise
    
    # Configuration - optimized for GPU
    BASE_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
    OUTPUT_DIR = "./models/finetuned_sbert"
    TRAIN_RATIO = 0.80  # 80/20 split
    
    # Split data into train (80%) and validation (20%)
    logger.info("Splitting data into train (80%) and validation (20%)...")
    random.seed(42)
    random.shuffle(examples)
    split_idx = int(len(examples) * TRAIN_RATIO)
    train_examples_list = examples[:split_idx]
    val_examples_list = examples[split_idx:]
    logger.info(f"‚úì Training pairs: {len(train_examples_list):,}")
    logger.info(f"‚úì Validation pairs: {len(val_examples_list):,}\n")
    
    # Adaptive batch size based on GPU availability
    if gpu_available:
        EPOCHS = 5
        BATCH_SIZE = 32  # Larger batch for GPU
        WARMUP_STEPS = 100
        logger.info("Using GPU-optimized configuration:")
        logger.info(f"  - Larger batch size (32 instead of 16)")
        logger.info(f"  - Expected training time: 2-5 minutes")
        logger.info(f"  - Speedup: ~10-20x vs CPU\n")
    else:
        EPOCHS = 5
        BATCH_SIZE = 16  # Smaller batch for CPU
        WARMUP_STEPS = 100
        logger.info("Using CPU configuration:")
        logger.info(f"  - Batch size: 16")
        logger.info(f"  - Expected training time: 30-60 minutes\n")
    
    logger.info(f"{'='*80}")
    logger.info("TRAINING CONFIGURATION")
    logger.info(f"{'='*80}")
    logger.info(f"Base Model:       {BASE_MODEL}")
    logger.info(f"Device:           {'GPU' if gpu_available else 'CPU'}")
    logger.info(f"Total Pairs:      {len(examples):,}")
    logger.info(f"Training Pairs:   {len(train_examples_list):,}")
    logger.info(f"Validation Pairs: {len(val_examples_list):,}")
    logger.info(f"Epochs:           {EPOCHS}")
    logger.info(f"Batch Size:       {BATCH_SIZE}")
    logger.info(f"Warmup Steps:     {WARMUP_STEPS}")
    logger.info(f"Output Dir:       {OUTPUT_DIR}")
    logger.info(f"{'='*80}\n")
    
    # Load base model
    logger.info("Loading base SBERT model...")
    model = SentenceTransformer(BASE_MODEL)
    
    # Move model to device (GPU or CPU)
    model = model.to(device)
    logger.info(f"‚úì Model loaded and moved to {device}")
    logger.info(f"  Embedding dimension: {model.get_sentence_embedding_dimension()}\n")
    
    # Prepare examples
    logger.info("Preparing training examples...")
    train_examples = [
        InputExample(texts=[ex['sent1'], ex['sent2']], label=ex['label'])
        for ex in train_examples_list
    ]
    logger.info(f"‚úì {len(train_examples)} training examples prepared\n")
    
    val_examples = [
        InputExample(texts=[ex['sent1'], ex['sent2']], label=ex['label'])
        for ex in val_examples_list
    ]
    logger.info(f"‚úì {len(val_examples)} validation examples prepared\n")
    
    # Create data loader - more workers on GPU
    num_workers = 4 if gpu_available else 0
    logger.info(f"Creating data loaders (workers={num_workers})...")
    train_dataloader = DataLoader(
        train_examples,
        shuffle=True,
        batch_size=BATCH_SIZE,
        num_workers=num_workers
    )
    val_dataloader = DataLoader(
        val_examples,
        shuffle=False,
        batch_size=BATCH_SIZE,
        num_workers=num_workers
    )
    logger.info(f"‚úì Training data loader: {len(train_dataloader)} batches")
    logger.info(f"‚úì Validation data loader: {len(val_dataloader)} batches\n")
    
    # Setup loss function
    logger.info("Setting up CosineSimilarityLoss...")
    train_loss = losses.CosineSimilarityLoss(model=model)
    logger.info("‚úì Loss function ready\n")
    
    # Setup evaluator
    from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
    logger.info("Setting up validation evaluator...")
    val_sentences1 = [ex['sent1'] for ex in val_examples_list]
    val_sentences2 = [ex['sent2'] for ex in val_examples_list]
    val_scores = [float(ex['label']) for ex in val_examples_list]
    evaluator = EmbeddingSimilarityEvaluator(
        sentences1=val_sentences1,
        sentences2=val_sentences2,
        scores=val_scores,
        batch_size=BATCH_SIZE,
        main_similarity=None,
        name='validation'
    )
    logger.info("‚úì Evaluator ready\n")
    
    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Train
    logger.info(f"{'='*80}")
    logger.info("STARTING FINE-TUNING")
    logger.info(f"{'='*80}\n")
    
    if gpu_available:
        logger.info("üöÄ GPU-ACCELERATED TRAINING")
        logger.info("Training with NVIDIA GPU for maximum speed...\n")
    else:
        logger.info("‚ö†Ô∏è  CPU TRAINING")
        logger.info("Training with CPU (slower). Consider setting up CUDA for 10-20x speedup.\n")
    
    start_time = datetime.now()
    
    # Train with GPU optimization
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=EPOCHS,
        warmup_steps=WARMUP_STEPS,
        evaluator=evaluator,
        evaluation_steps=len(train_dataloader),
        show_progress_bar=True,
        checkpoint_save_steps=len(train_dataloader),
        checkpoint_save_total_limit=1,
        use_amp=gpu_available,  # Automatic Mixed Precision for GPU
        amp_eval_cpu=True,
    )
    
    training_time = datetime.now() - start_time
    
    logger.info(f"\n{'='*80}")
    logger.info(f"‚úì TRAINING COMPLETED")
    logger.info(f"{'='*80}")
    logger.info(f"Time: {training_time}")
    logger.info(f"Device: {'GPU' if gpu_available else 'CPU'}")
    logger.info(f"Throughput: {len(examples) / training_time.total_seconds():.0f} pairs/sec\n")
    
    # Compute evaluation metrics
    logger.info(f"{'='*80}")
    logger.info("COMPUTING VALIDATION METRICS")
    logger.info(f"{'='*80}\n")
    
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
    import numpy as np
    
    # Move model to CPU for inference
    model = model.to('cpu')
    
    # Get predictions on validation set
    logger.info("Computing embeddings for validation set...")
    val_embeddings1 = model.encode(val_sentences1, convert_to_tensor=False, show_progress_bar=False)
    val_embeddings2 = model.encode(val_sentences2, convert_to_tensor=False, show_progress_bar=False)
    
    # Compute cosine similarity
    from scipy.spatial.distance import cosine
    similarities = []
    for emb1, emb2 in zip(val_embeddings1, val_embeddings2):
        sim = 1 - cosine(emb1, emb2)
        similarities.append(sim)
    
    # Convert to binary predictions using threshold 0.5
    threshold = 0.5
    predictions = [1 if sim > threshold else 0 for sim in similarities]
    ground_truth = [1 if score > 0.5 else 0 for score in val_scores]
    
    # Calculate metrics
    accuracy = accuracy_score(ground_truth, predictions)
    precision = precision_score(ground_truth, predictions, zero_division=0)
    recall = recall_score(ground_truth, predictions, zero_division=0)
    f1 = f1_score(ground_truth, predictions, zero_division=0)
    
    logger.info(f"\nüìä VALIDATION METRICS")
    logger.info(f"{'='*80}")
    logger.info(f"Threshold:  {threshold}")
    logger.info(f"Accuracy:   {accuracy:.4f} ({accuracy*100:.2f}%)")
    logger.info(f"Precision:  {precision:.4f} ({precision*100:.2f}%)")
    logger.info(f"Recall:     {recall:.4f} ({recall*100:.2f}%)")
    logger.info(f"F1 Score:   {f1:.4f}")
    logger.info(f"{'='*80}\n")
    
    # Save model
    logger.info(f"Saving fine-tuned model to {OUTPUT_DIR}...")
    model.save(OUTPUT_DIR)
    logger.info("‚úì Model saved\n")
    
    # Save metadata
    metadata = {
        "base_model": BASE_MODEL,
        "total_pairs": len(examples),
        "training_pairs": len(train_examples_list),
        "validation_pairs": len(val_examples_list),
        "train_validation_split": f"{TRAIN_RATIO*100:.0f}/{(1-TRAIN_RATIO)*100:.0f}",
        "similar_pairs": similar,
        "dissimilar_pairs": dissimilar,
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "warmup_steps": WARMUP_STEPS,
        "training_time_seconds": int(training_time.total_seconds()),
        "trained_on": datetime.now().isoformat(),
        "trained_on_gpu": gpu_available,
        "gpu_device": str(device) if gpu_available else "cpu",
        "embedding_dimension": model.get_sentence_embedding_dimension(),
        "metrics": {
            "accuracy": float(accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1_score": float(f1),
            "threshold": threshold
        },
        "status": "ready"
    }
    
    with open(os.path.join(OUTPUT_DIR, "metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    logger.info(f"{'='*80}")
    logger.info("FINE-TUNING COMPLETE ‚úÖ")
    logger.info(f"{'='*80}")
    logger.info(f"Model saved to:     {OUTPUT_DIR}")
    logger.info(f"Total pairs:        {len(examples):,}")
    logger.info(f"Training pairs:     {len(train_examples_list):,} (80%)")
    logger.info(f"Validation pairs:   {len(val_examples_list):,} (20%)")
    logger.info(f"Training time:      {training_time}")
    logger.info(f"Device used:        {'üöÄ GPU (Fast!)' if gpu_available else '‚ö†Ô∏è  CPU (Slow)'}")
    logger.info(f"Epochs trained:     {EPOCHS}")
    
    logger.info(f"\nüìä Expected Accuracy Improvement:")
    logger.info(f"  Base Model:       ~75-80%")
    logger.info(f"  Fine-Tuned Model: ~90-95%")
    
    logger.info(f"\nüöÄ Next Step:")
    logger.info(f"  Run: python main.py")
    logger.info(f"  Your app will automatically use the fine-tuned model!")
    logger.info(f"  Logs will show: '‚úì Fine-tuned model loaded successfully'")
    logger.info(f"{'='*80}\n")

if __name__ == "__main__":
    main()
